# ACT 3 â€” Monitoring, Health Probes & Resource Limits

> **Duration:** ~10 minutes  
> **Script:** `scripts/07-monitoring.sh`  
> **Wow Factor:** Production-grade observability + self-protection â€” zero YAML written from scratch  
> **Message:** *"Observability, health management ÎºÎ±Î¹ resource isolation Î´ÎµÎ½ ÎµÎ¯Î½Î±Î¹ add-ons. Î•Î¯Î½Î±Î¹ Î¼Î­ÏÎ¿Ï‚ Ï„Î¿Ï… platform."*

---

## ğŸ¯ Mental Model First

> ğŸ’¬ *"Î¤ÏÎ¯Î± Ï€ÏÎ¬Î³Î¼Î±Ï„Î± Ï€Î¿Ï… Î­Î½Î± production ÏƒÏÏƒÏ„Î·Î¼Î± Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± Î­Ï‡ÎµÎ¹ Î±Ï€ÏŒ Ï„Î·Î½ Ï€ÏÏÏ„Î· Î¼Î­ÏÎ±: Î½Î± Î¼ÎµÏ„ÏÎ¬ÎµÎ¹ Ï„Î¿Î½ ÎµÎ±Ï…Ï„ÏŒ Ï„Î¿Ï…, Î½Î± Î¾Î­ÏÎµÎ¹ Î±Î½ ÎµÎ¯Î½Î±Î¹ Ï…Î³Î¹Î­Ï‚, ÎºÎ±Î¹ Î½Î± Î¼Î·Î½ 'Ï„ÏÏÎµÎ¹' ÏŒÎ»Î¿Ï…Ï‚ Ï„Î¿Ï…Ï‚ Ï€ÏŒÏÎ¿Ï…Ï‚ Ï„Î¿Ï… node."*

---

## ğŸ–¥ï¸ PART 1 â€” ServiceMonitor: scraping our app with Prometheus

### Concept

The OpenShift monitoring stack (Prometheus + Alertmanager + Thanos) is **already running**. To scrape a workload, we register a `ServiceMonitor` â€” a custom resource that tells Prometheus *where* to pull metrics.

Our Quarkus app exposes Micrometer metrics at `/q/metrics` (Prometheus format) out of the box.

### Steps

**1. Show the live metrics endpoint:**

```bash
curl http://<route>/q/metrics | head -20
```

> ğŸ’¬ *"Î‘Ï…Ï„Î¬ Ï„Î± metrics Î²Î³Î±Î¯Î½Î¿Ï…Î½ Î±Ï…Ï„ÏŒÎ¼Î±Ï„Î± Î±Ï€ÏŒ Ï„Î¿ Quarkus/Micrometer. JVM, HTTP latency, GC â€” ÏŒÎ»Î± Î¼Î­ÏƒÎ±."*

---

**2. Enable user-workload monitoring (cluster-admin â€” once per cluster):**

```yaml
# ConfigMap: openshift-monitoring / cluster-monitoring-config
data:
  config.yaml: |
    enableUserWorkload: true
```

> ğŸ’¬ *"Î‘Ï…Ï„ÏŒ Î»Î­ÎµÎ¹ ÏƒÏ„Î¿ Prometheus: 'ÎºÎ¿Î¯Ï„Î± ÎºÎ±Î¹ Ï„Î± workloads Ï„Ï‰Î½ namespaces, ÏŒÏ‡Î¹ Î¼ÏŒÎ½Î¿ Ï„Î± cluster components'."*

---

**3. Apply the ServiceMonitor:**

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: ocp-demo-app
  labels:
    app: ocp-demo-app
spec:
  selector:
    matchLabels:
      app: ocp-demo-app
  endpoints:
    - port: 8080-tcp
      path: /q/metrics
      interval: 15s
```

> ğŸ’¬ *"ÎˆÎ½Î± Î¼Î¹ÎºÏÏŒ YAML ÎºÎ±Î¹ Î¿ Prometheus Î¾Î­ÏÎµÎ¹ Î½Î± scrape-Î¬ÏÎµÎ¹ Ï„Î·Î½ ÎµÏ†Î±ÏÎ¼Î¿Î³Î® Î¼Î±Ï‚ ÎºÎ¬Î¸Îµ 15 Î´ÎµÏ…Ï„ÎµÏÏŒÎ»ÎµÏ€Ï„Î±."*

**Console:** Navigate to **Observe â†’ Targets** â€” the app endpoint will appear within ~30 s.

**PromQL demo query:**
```promql
http_server_requests_seconds_count{namespace="ocp-demo"}
```

---

## ğŸ–¥ï¸ PART 2 â€” Liveness & Readiness Probes

### Concept

| Probe | Endpoint | Failure action |
|-------|----------|---------------|
| **Readiness** | `/q/health/ready` | Pod removed from Service â€” **no traffic received** |
| **Liveness** | `/q/health/live` | Container **killed and restarted** by kubelet |

Our Quarkus app implements both via MicroProfile Health:

```java
@Liveness  @ApplicationScoped
public static class AppLiveness implements HealthCheck { ... }  // /q/health/live

@Readiness @ApplicationScoped
public static class AppReadiness implements HealthCheck { ... } // /q/health/ready
```

### Steps

**1. Hit the endpoints live:**

```bash
curl http://<route>/q/health/live
curl http://<route>/q/health/ready
```

Expected response:
```json
{ "status": "UP", "checks": [{ "name": "app-live", "status": "UP" }] }
```

---

**2. Patch probes onto the Deployment:**

```yaml
livenessProbe:
  httpGet:
    path: /q/health/live
    port: 8080
  initialDelaySeconds: 10
  periodSeconds: 10
  failureThreshold: 3

readinessProbe:
  httpGet:
    path: /q/health/ready
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 5
  failureThreshold: 3
```

> ğŸ’¬ *"initialDelaySeconds Î´Î¯Î½ÎµÎ¹ Ï‡ÏÏŒÎ½Î¿ ÏƒÏ„Î¿ JVM Î½Î± Î¾ÎµÎºÎ¹Î½Î®ÏƒÎµÎ¹ Ï€ÏÎ¹Î½ Î±ÏÏ‡Î¯ÏƒÎ¿Ï…Î½ Î¿Î¹ ÎµÎ»Î­Î³Ï‡Î¿Î¹. Î‘Î½ Î±Ï€Î¿Ï„ÏÏ‡ÎµÎ¹ 3 Ï†Î¿ÏÎ­Ï‚ Ï„Î¿ liveness â€” restart. Î‘Î½ Î±Ï€Î¿Ï„ÏÏ‡ÎµÎ¹ Ï„Î¿ readiness â€” Î²Î³Î±Î¯Î½ÎµÎ¹ Î±Ï€ÏŒ Ï„Î¿ load-balancer."*

---

**3. What a failing liveness event looks like (`oc describe pod`):**

```
Warning  Unhealthy  Liveness probe failed: HTTP probe failed with statuscode: 503
Warning  Killing    Container failed liveness probe, will be restarted
Normal   Pulled     Successfully pulled image ...
Normal   Started    Started container
```

> ğŸ’¬ *"Î‘Ï…Ï„ÏŒ ÎµÎ¯Î½Î±Î¹ self-healing ÏƒÎµ ÎµÏ€Î¯Ï€ÎµÎ´Î¿ container â€” Ï‡Ï‰ÏÎ¯Ï‚ Î½Î± Ï‡ÏÎµÎ¹Î±ÏƒÏ„ÎµÎ¯ ÎºÎ±Î½ÎµÎ¯Ï‚ Î½Î± ÎºÎ¬Î½ÎµÎ¹ Ï„Î¯Ï€Î¿Ï„Î±."*

**Console:** Workloads â†’ Deployments â†’ `ocp-demo-app` â†’ YAML â†’ search `livenessProbe`

---

## ğŸ–¥ï¸ PART 3 â€” Resource Requests & Limits

### Concept

```
requests  â”€â”€â†’  guaranteed minimum  (scheduler uses this for placement)
limits    â”€â”€â†’  hard ceiling        (throttle CPU / OOMKill for memory)
```

| Without limits | With limits |
|----------------|-------------|
| Noisy-neighbour can starve node | Hard cap per container |
| Scheduler guesses placement | Scheduler makes optimal decisions |
| OOM kills random pods | OOMKill only the offending container |
| No visibility into consumption | Graphs show request vs actual |

### QoS Classes

| requests == limits | QoS class | Priority |
|--------------------|-----------|----------|
| âœ… Both set equal | **Guaranteed** | Highest â€” last to be evicted |
| Requests < limits | **Burstable** | Medium |
| Neither set | **BestEffort** | Lowest â€” first to be evicted |

### Steps

**1. Show current (empty) resources:**

```bash
oc get deployment ocp-demo-app -o jsonpath='{.spec.template.spec.containers[0].resources}'
```

---

**2. Patch requests + limits:**

```yaml
resources:
  requests:
    cpu: "100m"      # 0.1 core guaranteed at scheduling
    memory: "256Mi"  # 256 MiB reserved on the node
  limits:
    cpu: "500m"      # 0.5 core max â€” throttled above this
    memory: "512Mi"  # 512 MiB max â€” OOMKilled above this
```

> ğŸ’¬ *"100m CPU = 1/10 ÎµÎ½ÏŒÏ‚ core. Î‘Î½ Î· ÎµÏ†Î±ÏÎ¼Î¿Î³Î® Ï€ÏÎ¿ÏƒÏ€Î±Î¸Î®ÏƒÎµÎ¹ Î½Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÎµÎ¹ Ï€Î¬Î½Ï‰ Î±Ï€ÏŒ 500m â€” throttle. Î‘Î½ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÎµÎ¹ Ï€Î¬Î½Ï‰ Î±Ï€ÏŒ 512Mi RAM â€” OOMKill ÎºÎ±Î¹ restart."*

---

**3. Verify with `oc adm top`:**

```bash
oc adm top pod -n ocp-demo -l app=ocp-demo-app
```

**Console:** Observe â†’ Dashboards â†’ **Kubernetes / Compute Resources / Namespace (Pods)**  
â†’ Each pod bar shows actual vs requested vs limit.

---

**4. Show LimitRange / ResourceQuota (cluster admin guardrails):**

```bash
oc get limitrange -n ocp-demo
oc get resourcequota -n ocp-demo
```

> ğŸ’¬ *"ÎŸ cluster admin Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Î¿ÏÎ¯ÏƒÎµÎ¹ LimitRange â€” defaults ÎºÎ±Î¹ maxima Î±Î½Î¬ container â€” ÎºÎ±Î¹ ResourceQuota â€” ÏƒÏ…Î½Î¿Î»Î¹ÎºÏŒ budget Î±Î½Î¬ namespace. ÎˆÏ„ÏƒÎ¹ ÎºÎ±Î½Î­Î½Î±Ï‚ developer Î´ÎµÎ½ Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± 'Ï†Î¬ÎµÎ¹' Ï„Î¿Î½ cluster."*

---

## ğŸ“Œ Recap

| What we did | Why it matters |
|-------------|---------------|
| `ServiceMonitor` | Prometheus scrapes `/q/metrics` â†’ metrics in Observe â†’ Targets |
| Readiness probe (`/q/health/ready`) | Pod exits load-balancer when not ready â€” zero failed requests |
| Liveness probe (`/q/health/live`) | Kubelet auto-restarts stuck containers |
| `resources.requests` | Scheduler places pods optimally |
| `resources.limits` | Noisy containers are throttled / restarted, not neighbours |

---

## ğŸ”‘ Key Commands

```bash
# View ServiceMonitor
oc get servicemonitor -n ocp-demo

# Check probe config
oc get deployment ocp-demo-app -o jsonpath='{.spec.template.spec.containers[0].livenessProbe}'

# Live resource usage
oc adm top pod -n ocp-demo

# Namespace quota
oc get resourcequota,limitrange -n ocp-demo
```

---

## â¡ï¸ Next: [Scaling Out](../08-scaling/README.md)
